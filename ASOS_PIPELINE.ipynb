{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a46e4b7-0d87-44ff-bcdf-8bab809ed7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries required for the project\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "VEP_DIR = os.path.expanduser(\"/full/path/to/vep_data\") # add here your full path to vep_data\n",
    "TMP_DIR = os.path.join(VEP_DIR, \"tmp\")\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "FASTA_FILE = \"/full/path/to/GRCh38.fa\" # add here your full path to GRCh38.fa\n",
    "ASSEMBLY = \"GRCh38\"\n",
    "INPUT_VCF = \"/full/path/to/input.vcf\" # add here your full path to input vcf file\n",
    "PLUGINS_DIR = \"/full/path/to/Plugins\" # add here your full path to Plugins directory\n",
    "REVEL_FILE = f\"{PLUGINS_DIR}/new_tabbed_revel_grch38.tsv.gz\"\n",
    "MAXENTSCAN_FILE = f\"{PLUGINS_DIR}/fordownload\"\n",
    "SPLICEAI_SNV = f\"{PLUGINS_DIR}/spliceai_scores.raw.snv.hg38.vcf.gz\"\n",
    "SPLICEAI_INDEL = f\"{PLUGINS_DIR}/spliceai_scores.raw.indel.hg38.vcf.gz\"\n",
    "SPLICEVAULT_FILE = f\"{PLUGINS_DIR}/SpliceVault_data_GRCh38.tsv.gz\"\n",
    "ALPHAMISSENSE_FILE = f\"{PLUGINS_DIR}/AlphaMissense_hg38.tsv.gz\"\n",
    "LOFTEE_PATH = f\"{PLUGINS_DIR}/loftee\"\n",
    "HUMAN_ANCESTOR_FA = f\"{LOFTEE_PATH}/human_ancestor.fa\"\n",
    "CONSERVATION_FILE = f\"{LOFTEE_PATH}/phylocsf_gerp.sql\"\n",
    "PANGOLIN_SCRIPT = f\"{PLUGINS_DIR}/Pangolin/scripts/gencode.v38.annotation.db\"\n",
    "\n",
    "def run_vep_with_plugins():\n",
    "    output_file = f\"{VEP_DIR}/tmp/output1.vcf\"\n",
    "    command = [\n",
    "        \"singularity\", \"exec\", \"vep.sif\", \"vep\",\n",
    "        \"--dir\", VEP_DIR,\n",
    "        \"--cache\", \"--offline\",\n",
    "        \"--fasta\", FASTA_FILE,\n",
    "        \"--assembly\", ASSEMBLY,\n",
    "        \"--everything\", \"--format\", \"vcf\", \"--vcf\", \"--force_overwrite\",\n",
    "        \"-i\", INPUT_VCF, \"-o\", output_file,\n",
    "        \"--plugin\", f\"REVEL,file={REVEL_FILE}\",\n",
    "        \"--plugin\", f\"MaxEntScan,{MAXENTSCAN_FILE}\",\n",
    "        \"--plugin\", f\"SpliceAI,snv={SPLICEAI_SNV},indel={SPLICEAI_INDEL}\",\n",
    "        \"--plugin\", f\"SpliceVault,file={SPLICEVAULT_FILE}\",\n",
    "        \"--plugin\", f\"AlphaMissense,file={ALPHAMISSENSE_FILE}\"\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "def extract_fields_with_bcftools(input_file, output_file, fields):\n",
    "    command = [\n",
    "        \"/full/path/to/bcftools\", \"+split-vep\",  # add here your full path to bcftools\n",
    "        input_file, \"-f\", fields, \"-d\",\n",
    "        \"-o\", output_file\n",
    "    ]\n",
    "    subprocess.run(\" \".join(command), shell=True, check=True)\n",
    "\n",
    "def run_vep_with_loftee():\n",
    "    output_file = f\"{VEP_DIR}/tmp/output2.vcf\"\n",
    "    command = [\n",
    "        \"singularity\", \"exec\", \"vep.sif\", \"vep\",\n",
    "        \"--dir\", VEP_DIR,\n",
    "        \"--cache\", \"--offline\",\n",
    "        \"--fasta\", FASTA_FILE,\n",
    "        \"--assembly\", ASSEMBLY,\n",
    "        \"--everything\", \"--format\", \"vcf\", \"--vcf\", \"--force_overwrite\",\n",
    "        \"-i\", INPUT_VCF, \"-o\", output_file,\n",
    "        \"--plugin\", f\"LoF,loftee_path:{LOFTEE_PATH},human_ancestor_fa:{HUMAN_ANCESTOR_FA},conservation_file:{CONSERVATION_FILE}\",\n",
    "        \"--dir_plugins\", LOFTEE_PATH\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "    extract_fields_with_bcftools(\n",
    "        input_file=output_file,\n",
    "        output_file=f\"{VEP_DIR}/tmp/splitted_output2.csv\",\n",
    "        fields=\"'%Feature %LoF\\n'\"\n",
    "    )\n",
    "\n",
    "def run_bcftools_query(input_file, output_dir, fields):\n",
    "    output_file = output_dir + \"splitted_output3.vcf\"\n",
    "    command = [\"/full/path/to/bcftools\", \"query\", # add here your full path to bcftools\n",
    "               \"-f\", fields, input_file, \"-o\", output_file]\n",
    "    subprocess.run(\" \".join(command), shell=True, check=True)\n",
    "    \n",
    "def run_pangolin():\n",
    "    # Runs Pangolin for annotation\n",
    "    output_file = f\"{VEP_DIR}/tmp/pangolin_output\"\n",
    "    command = [\n",
    "        \"/full/path/to/pangolin\", INPUT_VCF, # add here your full path to pangolin\n",
    "        \"/full/path/to/GRCh38.primary_assembly.genome.fa.gz\", # add here your full path to GRCh38.primary_assembly.genome.fa.gz file\n",
    "        PANGOLIN_SCRIPT, output_file\n",
    "    ]\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "    run_bcftools_query(\n",
    "        input_file = output_file + \".vcf\",\n",
    "        output_dir = f\"{VEP_DIR}/tmp/\",\n",
    "        fields = \"'%POS %REF %ALT %INFO\\n'\"\n",
    "    )\n",
    "\n",
    "# Main execution\n",
    "run_vep_with_plugins()\n",
    "extract_fields_with_bcftools(\n",
    "    input_file=f\"{VEP_DIR}/tmp/output1.vcf\",\n",
    "    output_file=f\"{VEP_DIR}/tmp/splitted_output.csv\",\n",
    "    fields=\"'%CHROM %POS %REF %ALT %Existing_variation %SYMBOL %Gene %DOMAINS %Feature %MANE_SELECT %Consequence %IMPACT %HGVSc %HGVSp %CLIN_SIG %EXON %INTRON %REVEL %MaxEntScan_alt %MaxEntScan_diff %MaxEntScan_ref %SpliceAI_pred_DP_AG %SpliceAI_pred_DP_AL %SpliceAI_pred_DP_DG %SpliceAI_pred_DP_DL %SpliceAI_pred_DS_AG %SpliceAI_pred_DS_AL %SpliceAI_pred_DS_DG %SpliceAI_pred_DS_DL %SpliceVault_site_type %SpliceVault_top_events %am_pathogenicity\\n'\"\n",
    ")\n",
    "\n",
    "run_vep_with_loftee()\n",
    "run_pangolin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e2a20-cd9a-4111-994a-9fcf59669a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries required for the project\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "column_names=[\"chromosome\", \"Locus (hg38)\", \"Ref\", \"Alt\", \"Variant ID\", \"Gene\", \"Gene ID\", \"DOMAIN\", \"Transcript ID\", \"Mane Select\", \"Variant type\", \"Impact\", \"HGVSc\", \"HGVSp\", \"ClinVar interpretation\", \"Exon\", \"Intron\", \"REVEL\", \"MaxEntScan Alt\", \"MaxEntScan Diff\", \"MaxEntScan Ref\", \"SpliceAI AG position\", \"SpliceAI AL position\", \"SpliceAI DG position\", \"SpliceAI DL position\", \"SpliceAI AG score\", \"SpliceAI AL score\", \"SpliceAI DG score\", \"SpliceAI DL score\", \"SpliceVault site type\", \"SpliceVault top events\", \"AlphaMissense\"]\n",
    "df = pd.read_csv(f'{TMP_DIR}/splitted_output.csv',sep=' ',names=column_names)\n",
    "\n",
    "new_column_1 = df[\"Variant type\"].str.split(\"&\", n=4, expand=True)\n",
    "df.insert(5,'Variant Type',new_column_1[0])\n",
    "new_column_2 = df[\"Variant ID\"].str.split(\"&\", n=8, expand=True)\n",
    "df.insert(4,'Variant Id',new_column_2[0])\n",
    "new_column_3 = df[\"SpliceVault top events\"].str.split(\"&\", n=3, expand=True)\n",
    "df.insert(29,'SpliceVault Top Event',new_column_3[0])\n",
    "new_column_4 = df[\"Exon\"].str.split(\"/\", n=2, expand=True)\n",
    "df.insert(15,'EXON',new_column_4[0])\n",
    "df.insert(16,'EXON TOT',new_column_4[1])\n",
    "new_column_5 = df[\"Intron\"].str.split(\"/\", n=2, expand=True)\n",
    "df.insert(17,'INTRON',new_column_5[0])\n",
    "df.insert(18,'INTRON TOT',new_column_5[1])\n",
    "new_column_6 = df[\"DOMAIN\"].str.split(\"Pfam:\", n=2, expand=True)\n",
    "df.insert(7,'PFAM',new_column_6[1])\n",
    "new_column_7 = df[\"PFAM\"].str.split(\"&\", n=8, expand=True)\n",
    "df.insert(9,'PFAM_ACCESSION',new_column_7[0])\n",
    "\n",
    "del df[\"Variant type\"]\n",
    "del df[\"Variant ID\"]\n",
    "del df[\"SpliceVault top events\"]\n",
    "del df[\"Exon\"]\n",
    "del df[\"Intron\"]\n",
    "del df[\"PFAM\"]\n",
    "del df[\"DOMAIN\"]\n",
    "\n",
    "df['MaxEntScan Ref'] = pd.to_numeric(df['MaxEntScan Ref'], errors='coerce')\n",
    "df['MaxEntScan Alt'] = pd.to_numeric(df['MaxEntScan Alt'], errors='coerce')\n",
    "df['MaxEntScan Diff'] = pd.to_numeric(df['MaxEntScan Diff'], errors='coerce')\n",
    "df['REVEL'] = pd.to_numeric(df['REVEL'], errors='coerce')\n",
    "df['SpliceAI AG score'] = pd.to_numeric(df['SpliceAI AG score'], errors='coerce')\n",
    "df['SpliceAI AL score'] = pd.to_numeric(df['SpliceAI AL score'], errors='coerce')\n",
    "df['SpliceAI DG score'] = pd.to_numeric(df['SpliceAI DG score'], errors='coerce')\n",
    "df['SpliceAI DL score'] = pd.to_numeric(df['SpliceAI DL score'], errors='coerce')\n",
    "df['SpliceAI AG position'] = pd.to_numeric(df['SpliceAI AG position'], errors='coerce')\n",
    "df['SpliceAI AL position'] = pd.to_numeric(df['SpliceAI AL position'], errors='coerce')\n",
    "df['SpliceAI DG position'] = pd.to_numeric(df['SpliceAI DG position'], errors='coerce')\n",
    "df['SpliceAI DL position'] = pd.to_numeric(df['SpliceAI DL position'], errors='coerce')\n",
    "df['INTRON'] = pd.to_numeric(df['INTRON'], errors='coerce')\n",
    "df['INTRON TOT'] = pd.to_numeric(df['INTRON TOT'], errors='coerce')\n",
    "df['EXON'] = pd.to_numeric(df['EXON'], errors='coerce')\n",
    "df['EXON TOT'] = pd.to_numeric(df['EXON TOT'], errors='coerce')\n",
    "df['AlphaMissense'] = pd.to_numeric(df['AlphaMissense'], errors='coerce')\n",
    "\n",
    "df['REVEL'] = df['REVEL'].fillna(0)\n",
    "\n",
    "column_names2=[\"Transcript ID\",\"LoF\"]\n",
    "df1 = pd.read_csv(f'{TMP_DIR}/splitted_output2.csv',sep=' ', names=column_names2)\n",
    "df = pd.merge(df, df1, on=\"Transcript ID\")\n",
    "column_names3=[\"Locus (hg38)\",\"Ref\",\"Alt\",\"Pangolin scores\"]\n",
    "df2 = pd.read_csv(f'{TMP_DIR}/splitted_output3.vcf',sep=' ', names=column_names3)\n",
    "df = pd.merge(df, df2, on=\"Locus (hg38)\")\n",
    "df = df.rename(columns={'Ref_x': 'Ref', 'Alt_x': 'Alt'})\n",
    "del df[\"Ref_y\"]\t\n",
    "del df[\"Alt_y\"]\n",
    "\n",
    "df = df.loc[(df['Mane Select']!=\".\")].reset_index(drop=True)\n",
    "\n",
    "new_column_8 = df[\"Pangolin scores\"].str.split(\"|\", n=4, expand=True)\n",
    "df.insert(29,'Pangolin Splice gain (pos:score)',new_column_8[1])\n",
    "new_column_9 = df[\"Pangolin scores\"].str.split(\"|\", n=4, expand=True)\n",
    "df.insert(29,'Pangolin Splice loss (pos:score)',new_column_9[2])\n",
    "new_column_10 = df[\"Pangolin Splice gain (pos:score)\"].str.split(\":\", n=1, expand=True)\n",
    "df.insert(29,'Pangolin Splice gain score',new_column_10[1])\n",
    "new_column_11 = df[\"Pangolin Splice loss (pos:score)\"].str.split(\":\", n=1, expand=True)\n",
    "df.insert(29,'Pangolin Splice loss score',new_column_11[1])\n",
    "\n",
    "df['Pangolin Splice gain score'] = pd.to_numeric(df['Pangolin Splice gain score'], errors='coerce') \n",
    "df['Pangolin Splice loss score'] = pd.to_numeric(df['Pangolin Splice loss score'], errors='coerce')\n",
    "\n",
    "df_1 = df[(df['Variant Type']==\"intron_variant\")] \n",
    "df_1 = df_1.reset_index(drop=True)\n",
    "df_2 = df[(df['Variant Type']!=\"intron_variant\") & (df['Variant Type']!=\"frameshift_variant\") & (df['Variant Type']!=\"stop_gained\")] \n",
    "df_2 = df_2.reset_index(drop=True) \n",
    "df_3 = df[(df['Variant Type']==\"frameshift_variant\") | (df['Variant Type']==\"stop_gained\")]\n",
    "df_3 = df_3.reset_index(drop=True) \n",
    "\n",
    "r = []\n",
    "for i in range(len(df_3)):\n",
    "    if (df_3.loc[i, \"EXON\"] != 1 and df_3.loc[i, \"EXON\"] != df_3.loc[i, \"EXON TOT\"]):\n",
    "        r.append(\"Candidate\")\n",
    "    else:\n",
    "        r.append(\"Other\")  \n",
    "df_3['ASO'] = r\n",
    "\n",
    "tier = []\n",
    "for i in range(len(df_3)):\n",
    "    tier.append(\"Tier 2\")\n",
    "df_3['Tier'] = tier\n",
    "df_3.to_csv(f'{TMP_DIR}/frameshift_stopgain_variants.csv', sep='\\t', index=False)\n",
    "\n",
    "r1 = []\n",
    "for i in range(len(df_2)):\n",
    "    if (df_2.loc[i, \"Variant Type\"] == \"start_lost\" or \n",
    "       (df_2.loc[i, \"Variant Type\"] == \"missense_variant\" and  \n",
    "       (df_2.loc[i, \"REVEL\"] >= 0.5 or df_2.loc[i, \"AlphaMissense\"] >= 0.7))):\n",
    "        r1.append(\"SEVERE\")\n",
    "    elif (df_2.loc[i, \"Variant Type\"] == \"synonymous_variant\" and df_2.loc[i, \"REVEL\"] < 0.5) or (df_2.loc[i, \"Variant Type\"] == \"missense_variant\" and (df_2.loc[i, \"REVEL\"] < 0.5 or df_2.loc[i, \"AlphaMissense\"] < 0.7)):\n",
    "        r1.append(\"NO TO LITTLE\")\n",
    "    elif (df_2.loc[i, \"Variant Type\"] == \"synonymous_variant\" and df_2.loc[i, \"REVEL\"] >= 0.5):\n",
    "        r1.append(\"MODERATE\")\n",
    "    else:\n",
    "        r1.append(\".\")  \n",
    "df_2['Damage to coding function'] = r1  \n",
    "\n",
    "r2 = []\n",
    "for i in range(len(df_2)):\n",
    "    if (df_2.loc[i, \"SpliceAI DL score\"] >= 0.1 or df_2.loc[i, \"SpliceAI AL score\"] >= 0.1):\n",
    "        if (df_2.loc[i, \"MaxEntScan Alt\"] < df_2.loc[i, \"MaxEntScan Ref\"] and \n",
    "           (df_2.loc[i, \"MaxEntScan Alt\"] < 2 or \n",
    "           df_2.loc[i, \"MaxEntScan Alt\"] < 0.3 * df_2.loc[i, \"MaxEntScan Ref\"])):\n",
    "            r2.append(\"SEVERE\")\n",
    "        elif (df_2.loc[i, \"MaxEntScan Alt\"] < df_2.loc[i, \"MaxEntScan Ref\"] and \n",
    "             (df_2.loc[i, \"MaxEntScan Alt\"] >= 2 or \n",
    "             df_2.loc[i, \"MaxEntScan Alt\"] >= 0.3 * df_2.loc[i, \"MaxEntScan Ref\"])):\n",
    "            r2.append(\"MODERATE\")\n",
    "        else:\n",
    "            r2.append(\"NO TO LITTLE\")\n",
    "    else:\n",
    "        r2.append(\".\")\n",
    "df_2['Damage to canonical splicing'] = r2  \n",
    "\n",
    "r3 = []\n",
    "for i in range(len(df_2)):\n",
    "    if (df_2.loc[i, \"SpliceAI DG score\"] >= 0.1 or \n",
    "        df_2.loc[i, \"SpliceAI AG score\"] >= 0.1 or \n",
    "        df_2.loc[i, \"MaxEntScan Alt\"] >= 2 or \n",
    "        (df_2.loc[i, \"Pangolin Splice gain score\"] >= 0.2 and \n",
    "         df_2.loc[i, \"Pangolin Splice loss score\"] <= 0.2)):\n",
    "        r3.append(\"GAIN\") \n",
    "    elif ((df_2.loc[i, \"SpliceAI DL score\"] >= 0.1 or \n",
    "           df_2.loc[i, \"SpliceAI AL score\"] >= 0.1) and \n",
    "          (df_2.loc[i, \"SpliceAI DG score\"] < 0.1 or \n",
    "           df_2.loc[i, \"SpliceAI AG score\"] < 0.1) or \n",
    "          (df_2.loc[i, \"Pangolin Splice gain score\"] <= 0.2 and \n",
    "           df_2.loc[i, \"Pangolin Splice loss score\"] >= 0.2)):\n",
    "        r3.append(\"SKIPPING OR RETENTION\")   \n",
    "    else:\n",
    "        r3.append(\".\")\n",
    "df_2['Mis-splicing type'] = r3\n",
    "\n",
    "aso = []\n",
    "for i in range(len(df_2)):\n",
    "    if (df_2.loc[i, \"Damage to coding function\"] == \"NO TO LITTLE\" and \n",
    "        df_2.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "        df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"PROBABLY\")  # High likelihood of ASO amenability\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"POSSIBLY\")  # Moderate likelihood of ASO amenability\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"PROBABLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \".\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"PROBABLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \".\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \".\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \".\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"POSSIBLY\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \".\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso.append(\"AMENABLE\")\n",
    "    elif (df_2.loc[i, \"Damage to coding function\"] == \".\" and \n",
    "          df_2.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_2.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso.append(\"AMENABLE\")\n",
    "    else:\n",
    "        aso.append(\".\")\n",
    "df_2['ASO-amenability'] = aso\n",
    "\n",
    "aso1 = []\n",
    "for i in range(len(df_2)):\n",
    "    if df_2.loc[i, \"ASO-amenability\"] == \"POSSIBLY\" or df_2.loc[i, \"ASO-amenability\"] == \"PROBABLY\":\n",
    "        aso1.append(\"Candidate\") \n",
    "    else:\n",
    "        aso1.append(\"Other\")\n",
    "df_2['ASO'] = aso1\n",
    "\n",
    "tier = []\n",
    "for i in range(len(df_2)):\n",
    "    if (df_2.loc[i, \"Variant Type\"] == \"synonymous_variant\" and df_2.loc[i, \"REVEL\"] < 0.5 or \n",
    "        df_2.loc[i, \"Variant Type\"] != \"missense_variant\" and df_2.loc[i, \"REVEL\"] < 0.5):\n",
    "        tier.append(\"Tier 1\")\n",
    "    elif (df_2.loc[i, \"Variant Type\"] == \"synonymous_variant\" and \n",
    "          (df_2.loc[i, \"REVEL\"] >= 0.5 or df_2.loc[i, \"AlphaMissense\"] >= 0.5)):\n",
    "        tier.append(\"Tier 3\")\n",
    "    elif (df_2.loc[i, \"Variant Type\"] == \"missense_variant\" and \n",
    "          (df_2.loc[i, \"REVEL\"] < 0.5 or df_2.loc[i, \"AlphaMissense\"] < 0.5)):\n",
    "        tier.append(\"Tier 4\")\n",
    "    else:\n",
    "        tier.append(\"Tier 5\")\n",
    "df_2['Tier'] = tier\n",
    "\n",
    "df_2.to_csv(f'{TMP_DIR}/non_intron_variants.csv', sep='\\t', index=False)\n",
    "\n",
    "r4 = []\n",
    "for i in range(len(df_1)):\n",
    "    if (df_1.loc[i, \"SpliceAI DL score\"] >= 0.1 or df_1.loc[i, \"SpliceAI AL score\"] >= 0.1):\n",
    "        if (df_1.loc[i, \"MaxEntScan Alt\"] < df_1.loc[i, \"MaxEntScan Ref\"] and \n",
    "            (df_1.loc[i, \"MaxEntScan Alt\"] < 2 or \n",
    "             df_1.loc[i, \"MaxEntScan Alt\"] < 0.3 * df_1.loc[i, \"MaxEntScan Ref\"])):\n",
    "            r4.append(\"SEVERE\")\n",
    "        elif (df_1.loc[i, \"MaxEntScan Alt\"] < df_1.loc[i, \"MaxEntScan Ref\"] and \n",
    "              (df_1.loc[i, \"MaxEntScan Alt\"] >= 2 or \n",
    "               df_1.loc[i, \"MaxEntScan Alt\"] >= 0.3 * df_1.loc[i, \"MaxEntScan Ref\"])):\n",
    "            r4.append(\"MODERATE\")\n",
    "        else:\n",
    "            r4.append(\"NO TO LITTLE\")\n",
    "    else:\n",
    "        r4.append(\".\")\n",
    "df_1['Damage to canonical splicing'] = r4\n",
    "\n",
    "r5 = []\n",
    "for i in range(len(df_1)):\n",
    "    if (df_1.loc[i, \"SpliceAI DG score\"] >= 0.1 or df_1.loc[i, \"SpliceAI AG score\"] >= 0.1 or \n",
    "        df_1.loc[i, \"MaxEntScan Alt\"] >= 2 or \n",
    "        (df_1.loc[i, \"Pangolin Splice gain score\"] >= 0.2 and df_1.loc[i, \"Pangolin Splice loss score\"] <= 0.2)):\n",
    "        r5.append(\"GAIN\") \n",
    "    elif ((df_1.loc[i, \"SpliceAI DL score\"] >= 0.1 or df_1.loc[i, \"SpliceAI AL score\"] >= 0.1) and \n",
    "          (df_1.loc[i, \"SpliceAI DG score\"] < 0.1 or df_1.loc[i, \"SpliceAI AG score\"] < 0.1) or \n",
    "          (df_1.loc[i, \"Pangolin Splice gain score\"] <= 0.2 and df_1.loc[i, \"Pangolin Splice loss score\"] >= 0.2)):\n",
    "        r5.append(\"SKIPPING OR RETENTION\") \n",
    "    else:\n",
    "        r5.append(\".\")\n",
    "df_1['Mis-splicing type'] = r5\n",
    "\n",
    "aso2 = []\n",
    "for i in range(len(df_1)):\n",
    "    if (df_1.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "        df_1.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso2.append(\"PROBABLY\")\n",
    "    elif (df_1.loc[i, \"Damage to canonical splicing\"] == \"NO TO LITTLE\" and \n",
    "          df_1.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso2.append(\"POSSIBLY\")\n",
    "    elif (df_1.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_1.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso2.append(\"POSSIBLY\")\n",
    "    elif (df_1.loc[i, \"Damage to canonical splicing\"] == \"MODERATE\" and \n",
    "          df_1.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso2.append(\"POSSIBLY\")\n",
    "    elif (df_1.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_1.loc[i, \"Mis-splicing type\"] == \"SKIPPING OR RETENTION\"):\n",
    "        aso2.append(\"POSSIBLY\")\n",
    "    elif (df_1.loc[i, \"Damage to canonical splicing\"] == \".\" and \n",
    "          df_1.loc[i, \"Mis-splicing type\"] == \"GAIN\"):\n",
    "        aso2.append(\"POSSIBLY\")\n",
    "    else:\n",
    "        aso2.append(\".\")\n",
    "df_1['ASO-amenability'] = aso2\n",
    "\n",
    "aso3 = []\n",
    "for i in range(len(df_1)):\n",
    "    if (df_1.loc[i, \"ASO-amenability\"] == \"POSSIBLY\" or df_1.loc[i, \"ASO-amenability\"] == \"PROBABLY\"):\n",
    "        aso3.append(\"Candidate\")\n",
    "    else:\n",
    "        aso3.append(\"Other\")\n",
    "df_1['ASO'] = aso3\n",
    "\n",
    "tier1 = [\"Tier 1\"] * len(df_1)\n",
    "df_1['Tier'] = tier1\n",
    "df_1.to_csv(f'{TMP_DIR}/intron_variants.csv', sep='\\t', index=False)\n",
    "\n",
    "df_4 = df_1.loc[df_1['ASO'] == \"Candidate\"].reset_index(drop=True)\n",
    "df_5 = df_2.loc[df_2['ASO'] == \"Candidate\"].reset_index(drop=True)\n",
    "df_6 = df_3.loc[df_3['ASO'] == \"Candidate\"].reset_index(drop=True)\n",
    "result = pd.concat([df_4, df_5, df_6])\n",
    "\n",
    "columns_to_drop = [\"Pangolin scores\", \"Pangolin Splice loss (pos:score)\", \n",
    "                   \"Pangolin Splice gain (pos:score)\", \"Mane Select\", \n",
    "                   \"INTRON TOT\", \"ASO-amenability\", \"ASO\", \n",
    "                   \"Damage to canonical splicing\", \"Damage to coding function\"]\n",
    "result.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "column_names = list(result.columns.values)\n",
    "column_names.insert(33, column_names.pop(-1))\n",
    "result = result[column_names]\n",
    "pfam = pd.read_csv('full/path/to/pfam.csv', sep='\\t') ### add here your full path to pfam.csv\n",
    "pfam2 = pfam.drop_duplicates()\n",
    "result2 = pd.merge(result, pfam2, how=\"left\", on=[\"PFAM_ACCESSION\"])\n",
    "result2.to_csv(f'{TMP_DIR}/candidate_variants.csv', sep='\\t', index=False)\n",
    "\n",
    "(result2[\"Gene\"] + \":\" + result2[\"Transcript ID\"]).to_csv(f'{TMP_DIR}/gene_transcript.csv', sep='\\t', index=False)\n",
    "gene_transcript = pd.read_csv(f'{TMP_DIR}/gene_transcript.csv', sep='\\t')\n",
    "new22 = gene_transcript[\"0\"].str.split(\":\", n=1, expand=True)\n",
    "gene_transcript.insert(0, 'Gene', new22[0])\n",
    "gene_transcript.insert(1, 'Transcript ID', new22[1])\n",
    "del gene_transcript[\"0\"]\n",
    "\n",
    "gene_transcript.to_csv(f'{TMP_DIR}/gene_transcript2.csv',sep=\" \", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f764c7-c309-4c8b-ae0e-0f195faa8d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries required for the project\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_exon_coordinates_from_df(gene_transcript_df):\n",
    "    exon_data = []\n",
    "    for index, row in gene_transcript_df.iterrows():\n",
    "        gene_name = row['Gene']\n",
    "        transcript_id = row['Transcript ID']\n",
    "        server = \"https://rest.ensembl.org\"\n",
    "        ext = f\"/lookup/symbol/human/{gene_name}?expand=1\"\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        response = requests.get(server + ext, headers=headers)\n",
    "        if not response.ok:\n",
    "            response.raise_for_status()\n",
    "        data = response.json()\n",
    "        if 'Transcript' in data:\n",
    "            for transcript in data['Transcript']:\n",
    "                if transcript['id'] == transcript_id:\n",
    "                    i = 1\n",
    "                    for exon in transcript['Exon']:\n",
    "                        start = exon['start']\n",
    "                        end = exon['end']\n",
    "                        length = (end - start) + 1\n",
    "                        exon_data.append({\n",
    "                            'Gene': gene_name,\n",
    "                            'Transcript ID': transcript['id'],\n",
    "                            'EXON': i,\n",
    "                            'Exon Start': start,\n",
    "                            'Exon End': end,\n",
    "                            'Exon Length': length,\n",
    "                        })\n",
    "                        i += 1\n",
    "        else:\n",
    "            print(f\"No transcript found for gene: {gene_name}\")\n",
    "    \n",
    "    exon_df = pd.DataFrame(exon_data)\n",
    "    return exon_df\n",
    "\n",
    "gene_transcript_df = pd.read_csv('/full/path/to/gene_transcript2.csv', sep=' ') # add here your full path to gene_transcript2.csv\n",
    "exon_df = get_exon_coordinates_from_df(gene_transcript_df)\n",
    "exon_df.to_csv(f'{TMP_DIR}/gene_tra_exon.csv',sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5137bb8-5d29-42ea-8ffc-6950ef9d1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = pd.read_csv(f'{TMP_DIR}/gene_tra_exon.csv', sep=' ')\n",
    "ex2 = ex.drop_duplicates()\n",
    "\n",
    "ex2.to_csv(f'{TMP_DIR}/gene_transcript_exon_nodup.csv', sep=' ', index=False)\n",
    "ex2['mRNA Length'] = ex2['Exon Length'].groupby(ex2['Gene']).transform('sum')\n",
    "ex2.to_csv(f'{TMP_DIR}/gene_transcript_exon_nodup_mrnatot.csv', sep=' ', index=False)\n",
    "ex3 = pd.read_csv(f'{TMP_DIR}/gene_transcript_exon_nodup_mrnatot.csv', sep=' ')\n",
    "\n",
    "result3 = pd.merge(result2, ex2, how=\"left\", on=[\"Gene\", \"Transcript ID\", \"EXON\"])\n",
    "result3.to_csv(f'{TMP_DIR}/candidate_variants2.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632f0ebe-ff70-40a2-bef3-3789718a9088",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_ex = []\n",
    "len_ex = []\n",
    "i = 0\n",
    "\n",
    "while i < len(result3):\n",
    "    exon_skipping_result = \"No\"\n",
    "    exon_sum = 0\n",
    "    j = 0\n",
    "    \n",
    "    while j < len(ex3):\n",
    "        if (result3.loc[i, \"Gene\"] == ex3.loc[j, \"Gene\"] and \n",
    "            result3.loc[i, \"Transcript ID\"] == ex3.loc[j, \"Transcript ID\"] and \n",
    "            result3.loc[i, \"EXON\"] == ex3.loc[j, \"EXON\"]):\n",
    "            max_ex = math.ceil(min(10/100 * result3.loc[i, \"EXON TOT\"], result3.loc[i, \"EXON TOT\"] - result3.loc[i, \"EXON\"]))\n",
    "            y = 0\n",
    "            \n",
    "            while y < max_ex and j + y < len(ex3):\n",
    "                exon_sum += ex3.loc[j + y, \"Exon Length\"]\n",
    "                if (ex3.loc[j, \"mRNA Length\"] - exon_sum) % 3 == 0:\n",
    "                    exon_skipping_result = str(y+1) + \" exons\"\n",
    "                    len_ex.append(exon_sum)\n",
    "                    break\n",
    "                y += 1\n",
    "            \n",
    "            if exon_skipping_result != \"No\":\n",
    "                break\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "    sk_ex.append(exon_skipping_result)\n",
    "    \n",
    "    if exon_skipping_result == \"No\":\n",
    "        len_ex.append(0)\n",
    "    \n",
    "    i += 1\n",
    "\n",
    "result3['Exon Skipping'] = sk_ex\n",
    "result3['Exon Skipping Length'] = len_ex\n",
    "result4 = result3.drop_duplicates()\n",
    "result4.to_csv(f'{TMP_DIR}/candidate_variants3.csv', sep=' ', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a275b2ec-725e-47fa-887a-3e542f51ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_f = pd.read_csv(f'{TMP_DIR}/candidate_variants3.csv', sep=' ')\n",
    "df_p = pd.read_csv('full/path/to/poisonex_haploinsuf.csv',sep=',') ### add here your full path to poisonex_haploinsuf.csv\n",
    "\n",
    "dfp = pd.merge(res_f, df_p, how=\"left\", on=\"Gene\")\n",
    "\n",
    "del dfp[\"uORF\"]\n",
    "del dfp[\"NAT\"]\n",
    "del dfp[\"Pathogenic (ClinVar)\"]\n",
    "del dfp[\"Contains Known Splice Variants\"]\n",
    "del dfp[\"pLI\"]\n",
    "del dfp[\"Unnamed: 0\"]\n",
    "\n",
    "# Final Output\n",
    "dfp.to_csv(f'{TMP_DIR}/CANDIDATE_VARIANTS_POSION.csv', sep='\\t', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
